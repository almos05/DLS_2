{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0ADTojbpfLt"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\" width=\"400\"></p>\n",
    "\n",
    "# Домашнее задание. Обучение языковой модели с помощью LSTM (10 баллов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldHSmYY6p_mZ"
   },
   "source": [
    "Э\n",
    "В этом задании Вам предстоит обучить языковую модель с помощью рекуррентной нейронной сети. В отличие от семинарского занятия, Вам необходимо будет работать с отдельными словами, а не буквами.\n",
    "\n",
    "\n",
    "Установим модуль ```datasets```, чтобы нам проще было работать с данными."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3yvNdv6cp_0P",
    "ExecuteTime": {
     "end_time": "2025-03-17T18:01:51.524933Z",
     "start_time": "2025-03-17T18:01:51.521637Z"
    }
   },
   "source": "#!pip install datasets",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rh9ZXSeCpng9"
   },
   "source": [
    "Импорт необходимых библиотек"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XOJi16bLpd_O",
    "ExecuteTime": {
     "end_time": "2025-03-18T08:03:09.424793Z",
     "start_time": "2025-03-18T08:03:09.419475Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import seaborn\n",
    "seaborn.set(palette='summer')"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "91JuM0SQvXud",
    "ExecuteTime": {
     "end_time": "2025-03-18T08:03:05.751068Z",
     "start_time": "2025-03-18T08:03:05.481205Z"
    }
   },
   "source": [
    "nltk.download('punkt')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tende\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "adJC8ShFq9HM",
    "ExecuteTime": {
     "end_time": "2025-03-18T08:03:06.250017Z",
     "start_time": "2025-03-18T08:03:06.162016Z"
    }
   },
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwsfS1ENq5ig"
   },
   "source": [
    "## Подготовка данных\n",
    "\n",
    "Воспользуемся датасетом imdb. В нем хранятся отзывы о фильмах с сайта imdb. Загрузим данные с помощью функции ```load_dataset```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qHLNWOfJqSfc",
    "ExecuteTime": {
     "end_time": "2025-03-18T08:05:30.787195Z",
     "start_time": "2025-03-18T08:05:22.327151Z"
    }
   },
   "source": [
    "# Загрузим датасет\n",
    "dataset = load_dataset('imdb')\n",
    "dataset"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24gn7CuZ9agP"
   },
   "source": [
    "### Препроцессинг данных и создание словаря (1 балл)\n",
    "\n",
    "Далее вам необходмо самостоятельно произвести препроцессинг данных и получить словарь или же просто ```set``` строк. Что необходимо сделать:\n",
    "\n",
    "1. Разделить отдельные тренировочные примеры на отдельные предложения с помощью функции ```sent_tokenize``` из бибилиотеки ```nltk```. Каждое отдельное предложение будет одним тренировочным примером.\n",
    "2. Оставить только те предложения, в которых меньше ```word_threshold``` слов.\n",
    "3. Посчитать частоту вхождения каждого слова в оставшихся предложениях. Для деления предлоения на отдельные слова удобно использовать функцию ```word_tokenize```.\n",
    "4. Создать объект ```vocab``` класса ```set```, положить в него служебные токены '\\<unk\\>', '\\<bos\\>', '\\<eos\\>', '\\<pad\\>' и vocab_size самых частовстречающихся слов.   "
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T08:36:01.737759Z",
     "start_time": "2025-03-18T08:36:01.614446Z"
    }
   },
   "cell_type": "code",
   "source": "[word for word in sent_tokenize(dataset['train']['text'][0], 'english')]",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967.',\n",
       " 'I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life.',\n",
       " 'In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States.',\n",
       " 'In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic.',\n",
       " \"Really, the sex and nudity scenes are few and far between, even then it's not shot like some cheaply made porno.\",\n",
       " 'While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema.',\n",
       " 'Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America.',\n",
       " 'I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema.',\n",
       " \"But really, this film doesn't have much of a plot.\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ins2tVCdsS47",
    "ExecuteTime": {
     "end_time": "2025-03-18T09:03:52.463764Z",
     "start_time": "2025-03-18T09:03:47.750586Z"
    }
   },
   "source": [
    "sentences = []\n",
    "word_threshold = 32\n",
    "\n",
    "# Получить отдельные предложения и поместить их в sentences\n",
    "for text in tqdm(dataset['train']['text']):\n",
    "    sentences.extend([proposal.lower() for proposal in sent_tokenize(text, 'english') if len(proposal) < word_threshold])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [00:04<00:00, 5379.89it/s]\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bxeBxP3J1Rj3",
    "ExecuteTime": {
     "end_time": "2025-03-18T09:03:52.477254Z",
     "start_time": "2025-03-18T09:03:52.473763Z"
    }
   },
   "source": [
    "print(\"Всего предложений:\", len(sentences))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего предложений: 28827\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iT82XkT6ULA_"
   },
   "source": [
    "Посчитаем для каждого слова его встречаемость."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nEvCN0Y1w1yH",
    "ExecuteTime": {
     "end_time": "2025-03-18T09:06:22.597237Z",
     "start_time": "2025-03-18T09:06:21.638584Z"
    }
   },
   "source": [
    "words = []\n",
    "\n",
    "for proposal in sentences:\n",
    "    words.extend(word_tokenize(proposal, 'english'))\n",
    "\n",
    "words = Counter(words)\n",
    "words.most_common(10)\n",
    "# Расчет встречаемости слов"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 18593),\n",
       " ('!', 6816),\n",
       " ('it', 4088),\n",
       " ('?', 3781),\n",
       " ('the', 3227),\n",
       " ('is', 3217),\n",
       " ('i', 3109),\n",
       " ('a', 2818),\n",
       " (',', 2456),\n",
       " ('this', 2286)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4k4uSoHUSI0"
   },
   "source": [
    "Добавим в словарь ```vocab_size``` самых встречающихся слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oUBNwsK9xLIu"
   },
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "vocab_size = 40000\n",
    "\n",
    "# Наполнение словаря"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "assert '<unk>' in vocab\n",
    "assert '<bos>' in vocab\n",
    "assert '<eos>' in vocab\n",
    "assert '<pad>' in vocab\n",
    "assert len(vocab) == vocab_size + 4"
   ],
   "metadata": {
    "id": "ieT0DFUpXnV2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhACW2CQyck5"
   },
   "outputs": [],
   "source": [
    "print(\"Всего слов в словаре:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmeRYKSIUcdE"
   },
   "source": [
    "### Подготовка датасета (1 балл)\n",
    "\n",
    "Далее, как и в семинарском занятии, подготовим датасеты и даталоадеры.\n",
    "\n",
    "В классе ```WordDataset``` вам необходимо реализовать метод ```__getitem__```, который будет возвращать сэмпл данных по входному idx, то есть список целых чисел (индексов слов).\n",
    "\n",
    "Внутри этого метода необходимо добавить служебные токены начала и конца последовательности, а также токенизировать соответствующее предложение с помощью ```word_tokenize``` и сопоставить ему индексы из ```word2ind```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iD7SmSy3v2dl"
   },
   "outputs": [],
   "source": [
    "word2ind = {char: i for i, char in enumerate(vocab)}\n",
    "ind2word = {i: char for char, i in word2ind.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVzXL17PzC7K"
   },
   "outputs": [],
   "source": [
    "class WordDataset:\n",
    "    def __init__(self, sentences):\n",
    "        self.data = sentences\n",
    "        self.unk_id = word2ind['<unk>']\n",
    "        self.bos_id = word2ind['<bos>']\n",
    "        self.eos_id = word2ind['<eos>']\n",
    "        self.pad_id = word2ind['<pad>']\n",
    "\n",
    "    def __getitem__(self, idx: int) -> List[int]:\n",
    "        tokenized_sentence = []\n",
    "        # Допишите код здесь\n",
    "\n",
    "        return tokenized_sentence\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6CtYNMp2_g0"
   },
   "outputs": [],
   "source": [
    "def collate_fn_with_padding(\n",
    "    input_batch: List[List[int]], pad_id=word2ind['<pad>']) -> torch.Tensor:\n",
    "    seq_lens = [len(x) for x in input_batch]\n",
    "    max_seq_len = max(seq_lens)\n",
    "\n",
    "    new_batch = []\n",
    "    for sequence in input_batch:\n",
    "        for _ in range(max_seq_len - len(sequence)):\n",
    "            sequence.append(pad_id)\n",
    "        new_batch.append(sequence)\n",
    "\n",
    "    sequences = torch.LongTensor(new_batch).to(device)\n",
    "\n",
    "    new_batch = {\n",
    "        'input_ids': sequences[:,:-1],\n",
    "        'target_ids': sequences[:,1:]\n",
    "    }\n",
    "\n",
    "    return new_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6xmeK9Ys1BIG"
   },
   "outputs": [],
   "source": [
    "train_sentences, eval_sentences = train_test_split(sentences, test_size=0.2)\n",
    "eval_sentences, test_sentences = train_test_split(sentences, test_size=0.5)\n",
    "\n",
    "train_dataset = WordDataset(train_sentences)\n",
    "eval_dataset = WordDataset(eval_sentences)\n",
    "test_dataset = WordDataset(test_sentences)\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
    "\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, collate_fn=collate_fn_with_padding, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMAexY7Y45E4"
   },
   "source": [
    "## Обучение и архитектура модели\n",
    "\n",
    "Вам необходимо на практике проверить, что влияет на качество языковых моделей. В этом задании нужно провести серию экспериментов с различными вариантами языковых моделей и сравнить различия в конечной перплексии на тестовом множестве.\n",
    "\n",
    "Возмоэные идеи для экспериментов:\n",
    "\n",
    "* Различные RNN-блоки, например, LSTM или GRU. Также можно добавить сразу несколько RNN блоков друг над другом с помощью аргумента num_layers. Вам поможет официальная документация [здесь](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
    "* Различные размеры скрытого состояния. Различное количество линейных слоев после RNN-блока. Различные функции активации.\n",
    "* Добавление нормализаций в виде Dropout, BatchNorm или LayerNorm\n",
    "* Различные аргументы для оптимизации, например, подбор оптимального learning rate или тип алгоритма оптимизации SGD, Adam, RMSProp и другие\n",
    "* Любые другие идеи и подходы\n",
    "\n",
    "После проведения экспериментов необходимо составить таблицу результатов, в которой описан каждый эксперимент и посчитана перплексия на тестовом множестве.\n",
    "\n",
    "Учтите, что эксперименты, которые различаются, например, только размером скрытого состояния или количеством линейных слоев считаются, как один эксперимент.\n",
    "\n",
    "Успехов!"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Функция evaluate (1 балл)\n",
    "\n",
    "Заполните функцию ```evaluate```"
   ],
   "metadata": {
    "id": "KP1cO-3bmDv9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate(model, criterion, dataloader) -> float:\n",
    "    model.eval()\n",
    "    perplexity = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            logits = # Посчитайте логиты предсказаний следующих слов\n",
    "            loss = criterion(logits, batch['target_ids'].flatten())\n",
    "            perplexity.append(torch.exp(loss).item())\n",
    "\n",
    "    perplexity = sum(perplexity) / len(perplexity)\n",
    "\n",
    "    return perplexity"
   ],
   "metadata": {
    "id": "XUlMUVJ3mL4r"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLV63Vsk7loy"
   },
   "source": [
    "### Train loop (1 балл)\n",
    "\n",
    "Напишите функцию для обучения модели."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(...):\n",
    "    # Напишите код здесь"
   ],
   "metadata": {
    "id": "bSZmUC3YmocP"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Первый эксперимент (2 балла)\n",
    "\n",
    "Определите архитектуру модели и обучите её."
   ],
   "metadata": {
    "id": "hXmeyhBQmuq4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Опишите свою нейронную сеть здесь\n",
    "\n",
    "    def forward(self, input_batch: torch.Tensor) -> torch.Tensor:\n",
    "        # А тут опишите forward pass модели\n",
    "\n",
    "        return"
   ],
   "metadata": {
    "id": "qaWvqNJom0ij"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Обучите модель здесь"
   ],
   "metadata": {
    "id": "TxbEzn5fnBOY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Второй эксперимент (2 балла)\n",
    "\n",
    "Попробуйте что-то поменять в модели или в пайплайне обучения, идеи для экспериментов можно подсмотреть выше."
   ],
   "metadata": {
    "id": "X1EW4faIm0tl"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wkSE4jR1XzTg"
   },
   "outputs": [],
   "source": [
    "# Проведите второй эксперимент"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Отчет (2 балла)\n",
    "\n",
    "Опишите проведенные эксперименты. Сравните перплексии полученных моделей. Предложите идеи по улучшению качества моделей."
   ],
   "metadata": {
    "id": "Y5V9H3eoFeAu"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "M2GCDVeeF8LP"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
